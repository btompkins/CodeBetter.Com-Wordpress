<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Ian Cooper</title>
	<atom:link href="http://codebetter.com/iancooper/feed/" rel="self" type="application/rss+xml" />
	<link>http://codebetter.com/iancooper</link>
	<description>CodeBetter.Com - Stuff you need to Code Better!</description>
	<lastBuildDate>Thu, 06 Oct 2011 13:08:29 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>http://wordpress.org/?v=3.4.2</generator>
		<item>
		<title>Avoid Testing Implementation Details, Test Behaviours</title>
		<link>http://codebetter.com/iancooper/2011/10/06/avoid-testing-implementation-details-test-behaviours/</link>
		<comments>http://codebetter.com/iancooper/2011/10/06/avoid-testing-implementation-details-test-behaviours/#comments</comments>
		<pubDate>Thu, 06 Oct 2011 04:06:45 +0000</pubDate>
		<dc:creator>Ian Cooper</dc:creator>
				<category><![CDATA[ATDD]]></category>
		<category><![CDATA[BDD]]></category>
		<category><![CDATA[SOLID]]></category>
		<category><![CDATA[STDD]]></category>
		<category><![CDATA[TDD]]></category>
		<category><![CDATA[xUnit]]></category>

		<guid isPermaLink="false">http://codebetter.com/iancooper/?p=222</guid>
		<description><![CDATA[Every so often I return to Kent Beck&#8217;s Test-Driven Development. I honestly believe it to be one of the finest software development books ever written. What I love about the book is its simplicity. There is a sparseness to it&#160;&#8230; <a href="http://codebetter.com/iancooper/2011/10/06/avoid-testing-implementation-details-test-behaviours/">Continue&#160;reading&#160;<span class="meta-nav">&#8594;</span></a>]]></description>
			<content:encoded><![CDATA[<p>Every so often I return to <a href="http://en.wikipedia.org/wiki/Kent_Beck">Kent Beck&#8217;s</a> <a href="http://www.amazon.co.uk/Test-Driven-Development-Addison-Wesley-Signature/dp/0321146530">Test-Driven Development</a>. I honestly believe it to be one of the finest software development books ever written. What I love about the book is its simplicity. There is a sparseness to it that decieves, as though it were a lightweight exploration of the field. But continued reading as you progress with TDD reveals that it is fairly complete in its insights. Where later works have added pages and ideas, they have often lessened the technique, burdening it with complexity, born of misunderstandings of its power. I urge any of you who have practiced TDD for a while to re-read it regularly</p>
<p>For me one of the key ideas of TDD that is often overlooked is expressed in the cycle or Red, Green, Refactor. First we write a failing test, then we implement as quickly and dirtily as we can, and then we make it elegant. In this blog post I want to talk about some of the insights of the second and third steps that get missed. (Although you should remember the first step: tests don&#8217;t have tests so you need to prove them).</p>
<p>A lot of folks stall when they come to implement, because they try to implement well, thinking about good OO, SOLID patterns etc. Stop! The goal is to get green as soon as possible. Don&#8217;t try to build a decent solution at this point, just script out the quickest solution to the problem you can get too. Cut and Paste if you can, copy algorithms from CodeProject or StackOverflow if you can.</p>
<p>From <strong>Test-Driven Development</strong></p>
<blockquote><p>Stop. Hold on. I can hear the aesthetically inclined among you sneering and spitting. Copy-and-paste reuse? The death of abstraction? The killer of clean design?</p>
<p>If you&#8217;re upset, take a cleansing breath. In through the nose &#8230; hold it 1, 2, 3 &#8230; out through the mouth. There. Remember, our cycle has different phases (they go by quickly, often in seconds, but they are phases.):</p>
<p>1 Write a test.</p>
<p>2. Make it compile.</p>
<p>3. Run it to see that it fails.</p>
<p>4. Make it run.</p>
<p>5. Remove duplication.&#8221;</p></blockquote>
<p>It&#8217;s that final step (which is our refactoring step) in which we create Clean Code. Not before, but after.</p>
<blockquote><p>&#8220;Solving “clean code” at the same time that you solve “that works” can be too much to do at once. As soon as it is, go back to solving “that works,” and then “clean code” at leisure.&#8221;</p></blockquote>
<p>Now there is an important implication here that is often overlooked. We don&#8217;t have to write tests for the classes that we refactor out through that last step. They will be internal to our implementation, and are already covered by the first test. We do not need to add new tests for the next level &#8211; unless we feel we do not know how to navigate to the next step.</p>
<p>From <strong>Test-Driven Development</strong> again:</p>
<blockquote><p>&#8220;Because we are using Pairs as keys, we have to implement equals() and hashCode(). I&#8217;m not going to write tests for these, because we are writing this code in the context of a refactoring. If we get to the payoff of the refactoring and all of the tests run, then we expect the code to have been exercised. If I were programming with someone who didn&#8217;t see exactly where we were going with this, or if the logic became the least bit complex, I would begin writing separate tests.&#8221;</p></blockquote>
<p>Code developed in the context of refactoring does not require new tests! It is already covered, and safe refactoring techniques mean we should not be introducing specualtive change, just cleaning up the rough implementation we used to to green. At this point further tests help you steer (but they come at a cost)</p>
<p>The outcome of this understanding is that a test-case per class approach fails to capture the ethos for TDD. Adding a new class is not the trigger for writing tests. The trigger is implementing a requirement. So we should test outside-in, (though I would recommend using ports and adapters and making the &#8216;outside&#8217; the port), writing tests to cover then use cases (scenarios, examples, GWTs etc.), but only writing tests to cover the implementation details of that as and when we need to better understand the refactoring of the simple implementation we start with.</p>
<p>A positive outcome from this will be that much of our implementation will be internal or private, and never exposed outside our assembly. That will lower the coupling of our solution and make it easier for us to effect change.</p>
<p>From <strong>Test-Driven Development</strong> again:</p>
<blockquote><p>&#8220;In TDD we use refactoring in an interesting way. Usually, a refactoring cannot change the semantics of the program under any circumstances. In TDD, the circumstances we care about are the tests that are already passing.&#8221;</p></blockquote>
<p>When we refactor we don&#8217;t want to break tests. If our tests know too much about our implementation, that will be difficult, because changes to our implementation will necessarily result in us re-writing tests &#8211; at which point we are not refactoring. We would say that we have over-specified through our tests. Instead of assisting change, our tests have now begun to hamper it. As someone who has made this mistake, I can vouch for the fact that it is possible to write too many unit tests against details. If we follow TDD as originally envisaged though, the tests will naturally fall mainly on our public interface, not the implementation details, and we will find it far easier to meet the goal of changing the impementation details (safe change) and not the public interface (unsafe change)</p>
<p>Of course, this observation, that this idea has been overlooked by many TDD practitioners, formed the kernel of <a href="http://dannorth.net/introducing-bdd/">Dan North&#8217;s original BDD missive</a> and is why BDD focused on scenarios and outside-in for TDD</p>
]]></content:encoded>
			<wfw:commentRss>http://codebetter.com/iancooper/2011/10/06/avoid-testing-implementation-details-test-behaviours/feed/</wfw:commentRss>
		<slash:comments>15</slash:comments>
		</item>
		<item>
		<title>Why CRUD might be what they want, but may not be what they need</title>
		<link>http://codebetter.com/iancooper/2011/07/15/why-crud-might-be-what-they-want-but-may-not-be-what-they-need/</link>
		<comments>http://codebetter.com/iancooper/2011/07/15/why-crud-might-be-what-they-want-but-may-not-be-what-they-need/#comments</comments>
		<pubDate>Fri, 15 Jul 2011 13:35:56 +0000</pubDate>
		<dc:creator>Ian Cooper</dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://codebetter.com/iancooper/?p=194</guid>
		<description><![CDATA[A brief history of CRUD The Rolling Stones once sung: &#8220;You can&#8217;t always get what you want, But if you try sometimes, well you just might find, You get what you need&#8221; I think the same is true of CRUD.&#160;&#8230; <a href="http://codebetter.com/iancooper/2011/07/15/why-crud-might-be-what-they-want-but-may-not-be-what-they-need/">Continue&#160;reading&#160;<span class="meta-nav">&#8594;</span></a>]]></description>
			<content:encoded><![CDATA[<h2>A brief history of CRUD </h2>
<p>The Rolling Stones once sung:</p>
<blockquote><p>&#8220;You can&#8217;t always get what you want, But if you try sometimes, well you just might find, You get what you need&#8221;</p></blockquote>
<p>I think the same is true of CRUD. Our users seem to ask for it and we blindly agree to their wishes and build it; however, its regularly not what they need.</p>
<h3>The File Store</h3>
<p>When computers first arrived the early commercial use was to try and remove the need for paper files. Some of us may be old enough to remember that offices has filing departments with staff, where when we began a piece of work we went to get the records that the company had on that customer, or policy, or project. There were problems with this approach: file stores took up a lot of space, and the mangement of paper based records, particulary archival material is expensive; often, someone else had the file out, buried on their desk, and it was not always clear who &#8211; an early form of pessimistic concurrency control &#8211; which often stopped you responding to requests; files were only as useful as the discipline of maintaing them, and keeping them in order &#8211; searching was difficult within a file if it was not ordered and there was no idea of seaching the file store itself.</p>
<p>So it was natural when we began to think about ways in which computers could help in the workplace to think about electronic filing systems. We would record the data around the customer, policy, or project on an electronic form instead of a paper one and the computer could store it for us. We saved paper, we saved staff, we saved time, and we added new functionality such as the ability to search.</p>
<p>The UIs that we designed were optimized to enter data. After all there was a lot of data to enter just to catch up on. The essential questions for UX design became tab order or number of clicks &#8211; just how fast could we enter data. We wanted bulk data entry grids, and forms that mirrored our paper based forms, because that was what we needed to get out of the physical filing store and into our electronic store</p>
<p>Throughout the paper told us what to do and the process was built around the form. The form was passed from person to person. Stickers attached to the form, or boxes on the file defined the workflow. The paper-trail was the business process. Indeed, early analysis techniques simply followed the paper trail. Initial systems just followed the existing paper based process. Someone took the paper, the insurance application form, and type it in to the terminal. Then a batch run moved the record to the next step, so the next person in line could perform the same step that they had at the computer.</p>
<p>But no one really thought of this workflow as something the computer could handle. We just did handoffs the same way we did with paper. Indeed we sometimes used paper as the &#8216;token&#8217; that we passed around for the workflow and told you that you needed to enter the system and do some work. Or, later we became more sophisticated, and used email to do the same thing.</p>
<p>And for this perspective &#8211; the electronic filing store &#8211; Create, Read, Update, and Delete (CRUD) was the paradigm we needed. It described the lifecycle of a paper file perfectly and with our goal to automate that physical file store it was a capable model &#8211; or so it seemed.</p>
<h2>Brave New World</h2>
<p>Users tend to create electronic documents today. There is less paper. Inputs come from phone, email, the consumer directly accessing a web-based interface. Removing the phsyical file store is no longer behind the business case to provide IT systems. We no longer want to remove the old filing room and its associated clerks. Of course we might want to move away from our new shadow IT world of documents on shared drives and Excel spreadsheets.</p>
<p> The business goals around automation have changed too. To obtain the savings that justify the cost of IT savings, we want to enable smaller numbers of people to carry out the tasks formally done by many through not just electronic filing but through business process automation.</p>
<p> But once we shift the goals away from data entry to process automation, the shape of our architecture changes too. Less of the value comes from data entry today. It comes from providing a key business differentiator that allows us to outperform the competition in service, control of costs, management of supply chain, enabling new business models through disintermediation etc.</p>
<p>CRUD &#8211; a model for removing the old file store is less useful as a paradigm when we come to think about analysing and architecting a response to this kind of business process.</p>
<p>Think about how we force users to interact under a CRUD paradigm. They have procedures written in a manual, a checklist for the process. They have to edit this page,then edit that page etc. The system consists of CRUD screens where the users do the data entry. People run their workflow using email. Business process is the preserve of a priesthood who maintain their position by understanding &#8216;how things are done here&#8217;. </p>
<p>The problem here is that the behavior is not captured in the system, it is captured in the manual at best, more likely in the oral transmission of process from one worker to another. The business understands that it has workflow and behavior &#8211; they defined process with copies of memos, forms etc. We have not removed those steps by automating them, we have just  given them electronic forms. Pejoratively we are little better than the shadow IT offering written in Access; why engage an IT expert at all for that, all you do is add cost. </p>
<p>Much of the issue here is sheer laziness on our part. When I see tweets and blog posts saying: most business apps are CRUD, I hear, &#8220;we don&#8217;t want to work with the business to show them what IT could do for them&#8221;. Part of the reason is because it requires little skill to model CRUD; modelling business process is harder &#8211; but that is where the value lies. Think of early software development approaches that began with modeling the data, and then identified processes to act on that data. Is it any wonder we built CRUD based systems if we started by thinking about data. Data Flow diagrams, Process diagrams. These were the cornerstones of how we thought about the design phase of the software lifecycle and we have failed to escape from it. However, for our users recording data itself is rarely the end goal any more, completion of a business process is the goal.</p>
<p>Now sure, in some cases we might still have CRUD, for some kinds of reference data, and I&#8217;m sure a lot of people will respond to this with straw men of CRUD only applications. But be careful. In a lot of cases the customer thinks about records management, and they don&#8217;t know you could help them with processing, business rules, workflow (or are frightened of what that might mean for their jobs). Anywhere where there is a process in someone&#8217;s head, a word document, or excel spreadsheet that says, when this happens, you need to go into this page and fill in these values on the form, you have behavior, or intent that could and should be captured.</p>
<p>Let&#8217;s take a simple example: changing the state of a purchase order. What we tend to be saying is: the workflow for this process has reached a certain point and so it can move into this state. We can immediately see that we must have a number of things for this to occur: the purchase order must have some data that we need to validate it; we can also see that a number of things may be predicated on this occurring: we need to send the order to fulfilment, we need to begin the billing process etc. So this act, completing a purchase order, has both a workflow or business process and rules. A CRUD based approach to design tends to encourage us to leave these outside the system, with the user, in some set of manual checks or switching between parts of the application. What we want to look at is: is this process something we could automate, could the workflow and rules live in the system?</p>
<p>Of course a traditional objection to putting workflow and rules &#8216;in the system&#8217; is that they can change. This causes some folks to suggest that they ought to be left out.</p>
<p>Unfortunately in the typical business this means that a given named person Alice, Mary or Bob is the person that understands how to drive the process, and is mission critical to the success of the enterprise. If they get hit by a bus its all over, and they can pretty much ask for a raise whenever they like, because we can&#8217;t afford to lose them. Process diagrams often end up with a stick figure which imply that &#8216;Alice does her magic&#8217;. Often that person is uncommunicative and secretive about the process &#8211; to ensure that they remain essential to the business. When they are absent, things fall apart.</p>
<p>No business really wants that.
<p />
<p>And the reality is that we have plenty of options we can use to make workflow and rules more configurable &#8211; though we need to look at the tradeoffs between the cost to change the app against using a rules engine or workflow engine carefully by understanding the amount of likely change.</p>
<h2>Enter Tasks: What to you want to do today.</h2>
<p>We need to drop the focus on CRUD and move to a focus on tasks: what is it that the user wants to accomplish, and how can we help them accomplish that?</p>
<p>Let&#8217;s think about the case of a family doctor. In an appointment they tend to take the patient&#8217;s history, diagnose the illness, and prescribe a course of treatment. Now, in the UK the paper based way to achieve this used to be the <a href="http://www.patient.co.uk/doctor/Clinical-Negligence-and-the-Electronic-Patient-Record.htm">Lloyd George envelope.</a> Simply replicating the functionality of the Lloyd George envelope using a CRUD approach would give us screens for entering consultation notes, prescriptions&#8230; But there is a lot more we could do to help. If we treat diagnosis as a task, then we ought to consider searching previous medical history to see if anything shows up related to the patient&#8217;s complaint that may be of concern. There may be guidelines for these symptoms that we should be following to check that we are not missing illnesses like meningitis. When we prescribe we can list common drugs for the illness, and look for incompatibility with drugs that the patient is currently being prescribed in a drugs database. That may lead to us offering further advice, for example if the patient is using the contraceptive pill and we prescribe them antibiotics.</p>
<p>Or perhaps consider an on line retailer where the customer can go in and change their account details. If the customer changes their address, we might want to prompt the customer for in-flight orders and ask where they want them to be delivered &#8211; the old address or the new address.</p>
<p>Ask yourself the question: am I thinking about the intent of what the user wishes to accomplish?</p>
<p>The grid is particularly dangerous ground in this regard. It is rare that you can capture intent through a grid. if you are using a grid you are probably not focused on a task but on CRUD</p>
<p>Greg Young has a great <a href="http://cqrsinfo.com/documents/task-based-ui/">discussion on the Task-based UI</a>. It is one of the building blocks in a journey towards understanding the why of <a href="http://cqrsinfo.com/">CQRS</a></p>
]]></content:encoded>
			<wfw:commentRss>http://codebetter.com/iancooper/2011/07/15/why-crud-might-be-what-they-want-but-may-not-be-what-they-need/feed/</wfw:commentRss>
		<slash:comments>23</slash:comments>
		</item>
		<item>
		<title>Why use the command processor pattern in the service layer</title>
		<link>http://codebetter.com/iancooper/2011/04/27/why-use-the-command-processor-pattern-in-the-service-layer/</link>
		<comments>http://codebetter.com/iancooper/2011/04/27/why-use-the-command-processor-pattern-in-the-service-layer/#comments</comments>
		<pubDate>Wed, 27 Apr 2011 17:00:17 +0000</pubDate>
		<dc:creator>Ian Cooper</dc:creator>
				<category><![CDATA[CQRS]]></category>
		<category><![CDATA[DDD]]></category>
		<category><![CDATA[Events]]></category>
		<category><![CDATA[Object-Orientation]]></category>
		<category><![CDATA[SOLID]]></category>
		<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://codebetter.com/iancooper/?p=175</guid>
		<description><![CDATA[Using a Command Processor When we think about a layered or hexagonal architecture it is common to identify the need for a service layer. The service layer both provides a facade over our domain layer to applications &#8211; acting as&#160;&#8230; <a href="http://codebetter.com/iancooper/2011/04/27/why-use-the-command-processor-pattern-in-the-service-layer/">Continue&#160;reading&#160;<span class="meta-nav">&#8594;</span></a>]]></description>
			<content:encoded><![CDATA[<h4>Using a Command Processor</h4>
<p>When we think about a <a href="http://domaindrivendesign.org/node/118">layered </a>or <a href="http://alistair.cockburn.us/Hexagonal+architecture">hexagonal </a>architecture it is common to identify the need for a service layer. The service layer both provides a facade over our domain layer to applications &#8211; acting as an API definition &#8211; and contains the co-ordination and control logic for orchestrating how we respond to requests.</p>
<p>One option to implement this is the notion of a service as a class:</p>
<pre><code>
public class MyFatDomainService
{
    public void CreateMyThing(CreateMyThingCommand createMyThingCommand)
    {
        /*Stuff*/
    }

    public void UpdateMyThingForFoo(FooCommand fooHappened)
    {
       /*Other Stuff*/
    }

     public void UpdateMyThingForBar(BarCommand barHappened)
     {
        /*Other Stuff*/
     }

     /*Loads more of these*/
}
</code></pre>
<p>Another option is to use the <a href="wiki.hsr.ch/APF/files/CommandProcessor.pdf">command processor pattern</a>. There are some keys to understanding the choice to use a command processor to implement your service layer.</p>
<p>The <a href="http://www.objectmentor.com/resources/articles/isp.pdf">Interface Segregation Principle</a> states that clients should not be forced to depend on methods on an interface that they do not use. This is because we do not want to update the client because the interface changes to service other clients in a way that the client itself does not care about. <em>Operation script</em> style domain service classes force consumers (for example MVC controllers) to become dependent on methods on the domain service class that they do not consume.</p>
<p>Now this can be obviated by having the domain service implement a number of interfaces, and hand to its clients interfaces that only cover the concerns they have. With application service layers this naturally tends towards one method per interface.</p>
<pre><code>
public interface ICreateMyThingDomainService
{
    void CreateMyThing(CreateMyThingCommand createMyThingCommand);
}

public interface IUpdateMyThingForFooDomainService
{
    void UpdateMyThingForFoo(FooCommand fooHappened);
}

public interface IUpdateMyThingForBarDomainService
{
   void UpdateMyThingForBar(BarCommand barHappened);
}

public class MyFatDomainService : ICreateMyThingDomainService, IUpdateMyThingForFooDomainService, IUpdateMyThingForBarDomainService
{
   public void CreateMyThing(CreateMyThingCommand createMyThingCommand)
  {
      /*Stuff*/
   }

   public void UpdateMyThingForFoo(FooCommand fooHappened)
  {
      /*Other Stuff*/
  }

   public void UpdateMyThingForBar(BarCommand barHappened)
   {
      /*Other Stuff*/
    }

/*Loads more of these*/

}
</code></pre>
<p>Now the <a href="http://www.objectmentor.com/resources/articles/srp.pdf">Single Responsibility Principle</a> suggests that a class should have one and only one reason to change. All these separate interfaces begin to suggest that a separate class might be better for each interface, to avoid updating a class for concerns that it does not have.</p>
<pre><code>
public interface ICreateMyThingDomainService
{
    void CreateMyThing(CreateMyThingCommand createMyThingCommand);
}

public class CreateMyThingDomainService : ICreateMyThingDomainService
{
    public void CreateMyThing(CreateMyThingCommand createMyThingCommand)
    {
       /*Stuff */
    }
}

public interface IUpdateMyThingForFooDomainService
{
    void UpdateMyThingForFoo(FooCommand fooHappened);
}

public class UpdateMyThingForFooDomainService : IUpdateMyThingForBarDomainService
{
    public void UpdateMyThingForBar(BarCommand barHappened)
   {
      /*Other Stuff*/
   }
}

public interface IUpdateMyThingForFooDomainService
{
   void UpdateMyThingForBar(FooCommand barHappened);
}

public class UpdateMyThingForFooDomainService : IUpdateMyThingForFooDomainService
{
   public void UpdateMyThingForFoo(FooCommand barHappened)
  {
    /*Other Stuff*/
  }
}
</code></pre>
<p>Having split these individual classes out we might choose to avoid calling them directly, but instead decide to send a message to them. There are a number of reasons for this.</p>
<p>The first is that we decouple the caller from the service. This is useful where we might want to change what the service does – for example handle requests asynchronously, without modifying the caller.</p>
<p>The second is that we might want to handle orthogonal concerns orthogonally such as transactions or logging. Although we could use a technology like PostSharp to weave in aspects, a simpler approach is pipes and filters – create a pipeline with a series of steps, the end of which is our service. We then send a message, the framework instantiates a pipeline with those orthogonal concerns, the message is passed through the filter steps before arriving at the target handler.</p>
<p>In order to make this work the pipeline steps need to implement a common interface as this allows us to add <a href="http://en.wikipedia.org/wiki/Decorator_pattern">Decorators</a> that implement the same interface, forming a <a href="http://en.wikipedia.org/wiki/Chain-of-responsibility_pattern">Chain of Responsibility</a>.</p>
<p>&nbsp;</p>
<pre><code>
public interface IHandleMessages
{
    void Handle(T command);
}

public class CreateMyThingHandler : IHandleMessages
{
    public void Handles(CreateMyThingCommand createMyThingCommand)
   {
      /*Stuff */
  }
}

public class UpdateMyThingForFooHandler : IHandleMessages
{
    [Logging]
    public void Handles(BarCommand barHappened)
    {
      /*Other Stuff*/
    }
}

public class Logger : IHandleMessages
{
    public void Handles(BarCommand barHappened)
   {
       /*Other Stuff*/
   }
}
</code></pre>
<p>The code is not shown here, but you may want to use attributes to document a handler and show the decorators in the chain of responsibility. You could use these to decide what decorators to put in the chain. However, It&#8217;s worth noting that real implementations that assemble a chain of responsibility often use an Inversion of Control container to resolve the handler and you will often find it easier to implement if you have an IDecorate interface. Some of the pipeline building code involves usually involves dealing with <a href="http://msdn.microsoft.com/en-us/library/system.type.makegenerictype.aspx">Type.MakeGenericType</a> and you might want to be aware of that if you want to use this approach. I might do a separate post on building a chain of responsibility at a future date.</p>
<p>Note that this also serves to reduce the number of interfaces that we must implement as the generic interface can stand in for most of them.</p>
<p>Third is the number of dependencies our service has. We might have an action that we need to execute as part of the response to a number of actions by the user. We don’t want to repeat that code in every service. But, we also don’t want to have services calling too many other services, (that in turn may call other services), because we will get an explosion of dependencies for our service, and the service that it depends on. This makes it hard to get our service under test, or makes the tests unintelligible and results in <a href="http://altnetseattle.pbworks.com/w/page/12367942/Why%20We%20Stopped%20Using%20the%20Auto-Mocking%20Container%20and%20What%27s%20Next">anti-patterns like auto-mocking</a>. The need for auto-mocking may be seen as a design smell: you have too many dependencies; resolution might be to use a command processor.</p>
<p>We gain some dependency advantages from the split into separate handlers, because each handler will have fewer dependencies than a service. But in addition we can separate concerns in our handlers, such that we focus on updating a small part of our domain model or object graph in each handler (in DDD terms we focus on an <a href="http://domaindrivendesign.org/node/88">aggregate</a>).</p>
<p>So to further limit the number of dependencies we prefer to publish a message (an event) from the service that handles the initial request, handing off any work that is related to handling the command, but applies to a different aggregate (and should potentially be in a different transaction and consistency boundary). This allows the framework to pass the message to those services, removing our dependency on them.</p>
<pre><code>
public class CreateMyThingHandler : IHandleMessages
{
    IProcessCommands _commandProcessor;
    IMyThingRepository _myThingRepository;

    public CreateMyThingHandler(IProcessCommands commandProcessor, 
         IMyThingRepository myThingRepository)
   {
      _commandProcessor = commandProcessor;
      _myThingRepository = myThingRepository;
   }

   public void Handles(CreateMyThingCommand createMyThingCommand)
  {
      /*Use Factory or Factory Method to create a my thing  */
      /*save my thing to a repository*/

      _commandProcessor.Publish(
          new MyThingCreated
              {/* properties that other consumers may care about*/}
          );
   }
}
</code></pre>
<p>In addition a command processor, because it is responsible for loading the services, is also the point at which an Inversion of Control (IOC) container is injected into our request handling pipeline, which removes any issues with lack of support for IOC containers in legacy platforms like WebForms.</p>
<p>Finally switching to a command processor eases our path to handling some requests asynchronously.</p>
<h4>Disadvantages of using a command processor</h4>
<p>One common criticism of using a command processor is that it adds a level of indirection. To read the code you have to identify how commands map to handlers. Naming conventions can help ease the burden, especially if used with tools like R#.</p>
]]></content:encoded>
			<wfw:commentRss>http://codebetter.com/iancooper/2011/04/27/why-use-the-command-processor-pattern-in-the-service-layer/feed/</wfw:commentRss>
		<slash:comments>14</slash:comments>
		</item>
		<item>
		<title>Why Repository SaveUpdate is a smell</title>
		<link>http://codebetter.com/iancooper/2011/04/12/repository-saveupdate-is-a-smell/</link>
		<comments>http://codebetter.com/iancooper/2011/04/12/repository-saveupdate-is-a-smell/#comments</comments>
		<pubDate>Tue, 12 Apr 2011 05:30:40 +0000</pubDate>
		<dc:creator>Ian Cooper</dc:creator>
				<category><![CDATA[.Net]]></category>
		<category><![CDATA[DDD]]></category>
		<category><![CDATA[NHibernate]]></category>
		<category><![CDATA[ORM]]></category>

		<guid isPermaLink="false">http://codebetter.com/iancooper/?p=165</guid>
		<description><![CDATA[One of the idioms I see a fair amount of is the SaveUpdate method on a Repository. The intent is usually to persist an item to storage, using a SaveUpdate method to lazily avoid the question of whether it is&#160;&#8230; <a href="http://codebetter.com/iancooper/2011/04/12/repository-saveupdate-is-a-smell/">Continue&#160;reading&#160;<span class="meta-nav">&#8594;</span></a>]]></description>
			<content:encoded><![CDATA[<p>One of the idioms I see a fair amount of is the <strong>SaveUpdate </strong>method on a <a href="http://martinfowler.com/eaaCatalog/repository.html">Repository</a>. The intent is usually to persist an item to storage, using a <strong>SaveUpdate </strong>method to lazily avoid the question of whether it is a new object, or just one you are updating.</p>
<p>I have a couple of issues with this approach. The first is that a Repository is intended to have collection semantics and encapsulate the method used to obtain the data. Adding <strong>SaveUpdate</strong> makes it explicit that your repository is backed with some kind of persistent storage &#8211; why else would you need to save it. But a Repository does not encapsulate access to a Db, a repository is simply a collection of entities (in DDD aggregate roots) which is agnostic about where the entities are stored. A Repository might be entirely in memory, it might abstract a <a href="http://en.wikipedia.org/wiki/Representational_State_Transfer">REST API</a> on a separate service, then options are legion.</p>
<p>In most cases I would prefer that we have a similar interface to a collection: an <strong>Add</strong> method because I want to add new items into the collection, but not an update, because I should only need to change an object referenced from the collection for that.</p>
<p>Working with <a href="http://nhforge.org/Default.aspx">NHibernate </a>(NH) this also works with NH&#8217;s paradigm rather than against it. This is my second issue: you should rarely need to ever call SaveUpdate in NH, and only rarely call Save.</p>
<p>NH distinguishes between two types of objects: <em>transient </em>and <em>persistent</em>. A <em>persistent</em> object is one that is in the Db, a <em>transient </em>one is in memory. When you <strong>Save </strong>against an NH session a <em>transient </em>object becomes a <em>persistent </em>one. When an object is loaded from the Db it is a <em>persistent </em>object. The key to this is the <a href="http://martinfowler.com/eaaCatalog/identityMap.html">Identity Map</a> pattern &#8211; NH has a list of all the objects loaded from the Db, so that it knows to serve them back up to you, if you ask for the object representing a row again during the session. The idea is that you want to use the version with any changes you have already made, not a fresh copy from the Db.</p>
<p>When NH flushes a session, or commits a transaction, it does a dirty check for all the <em>persistent </em>objects in your Identity Map, and saves any changes. That&#8217;s it, no need to call <strong>SaveUpdate</strong>, just flush the session or commit a transaction and your <em>persistent </em>objects will be saved. So <strong>Update </strong>is meaningless to call explicitly, NH will do it for you. (Understanding this is important, because auto-flush will commit dirty writes when your session terminates, even if you threw an exception; this may not be what you desired, in which case you should not autoflush but rely on explicit transactions).</p>
<p>This fits nicely with our Repository as collection metaphor: we don&#8217;t need to do anything with objects that already part of the collection post-updating them.</p>
<p>In addition NH supports <em>persistence by cascade</em> &#8211; you know those options you set on many-to-one, many&#8211;to-many relationships etc. Provided your cascade options require NH to follow a relationship, a <em>persistent </em>object will also save children in its object graph. Now this does not make any difference for already <em>persistent </em>objects, but <em>transient </em>ones that can be reached by following a cascade will be saved too.</p>
<p>This means that you do not need to call <strong>Save </strong>for an object that is added to the graph of an already <em>persistent </em>the dirty check will pick it up &#8211; so adding a new object in this case does not require a call to <strong>Save</strong>.</p>
<p>This makes sense for our Repository pattern, we don&#8217;t tend to call <strong>Add </strong>for the children in the object graph of entities we add to a collection, and there. It is also worth noting that this lends itself to the DDD idea of an Aggregate Root being what a Repository loads, we need to load root objects, not all their children. (Sadly NH does not really support the notion of a <a href="http://martinfowler.com/eaaCatalog/coarseGrainedLock.html">coarse-grained lock</a>, which aggregates try to implement, <a href="http://ayende.com/Blog/archive/2009/05/30/nhibernate-ndash-coarse-grained-locks.aspx">but that&#8217;s a seperate issue</a>).</p>
<p>This means that we only ever need to <strong>Save </strong>i.e. <strong>Add </strong>to our Repository, on new <em>transient</em> objects that have no association with existing objects (in DDD terms likely to be Aggregate Roots). There are usually a lot less of these than other kinds of entities. So once you challenge the idea of doing <strong>SaveUpdate</strong>, you naturally begin to move toward thinking about a Repository loading an object graph, and not an entity. By doing this you are both encapsulating your access to whatever backs the repository, and in the case of an O<a href="http://en.wikipedia.org/wiki/Object-relational_mapping">bject-Relational Mapper </a>(ORM) working with the ORM. A good ORM is intended to support the OO paradigm, if you don&#8217;t exploit its power to work seamlessly, but rely on old school explicitly CRUD operations on a <a href="http://martinfowler.com/eaaCatalog/dataMapper.html">Data Mapper</a> then you miss some of the strengths of a well written ORM.</p>
<p>If you want to have explicit knowledge of the Db, and find it more comfortable to know that you are saving to the Db, use the Data Mapper pattern, not a Repository. Otherwise, consider that explicitly calling <strong>SaveUpdate </strong>(or synonyms of that) may well be a smell that you are pusing too much information about how persistence is done into your model.</p>
]]></content:encoded>
			<wfw:commentRss>http://codebetter.com/iancooper/2011/04/12/repository-saveupdate-is-a-smell/feed/</wfw:commentRss>
		<slash:comments>5</slash:comments>
		</item>
		<item>
		<title>Mocks and Tell Don&#8217;t Ask</title>
		<link>http://codebetter.com/iancooper/2011/04/05/tell-dont-ask-mocks/</link>
		<comments>http://codebetter.com/iancooper/2011/04/05/tell-dont-ask-mocks/#comments</comments>
		<pubDate>Tue, 05 Apr 2011 12:19:00 +0000</pubDate>
		<dc:creator>Ian Cooper</dc:creator>
				<category><![CDATA[.Net]]></category>
		<category><![CDATA[Agile]]></category>
		<category><![CDATA[BDD]]></category>
		<category><![CDATA[Behavior Specification]]></category>
		<category><![CDATA[Mocks]]></category>
		<category><![CDATA[Object-Orientation]]></category>
		<category><![CDATA[Stubs]]></category>
		<category><![CDATA[TDD]]></category>

		<guid isPermaLink="false">/blogs/ian_cooper/archive/2010/11/16/tell-don-t-ask-mocks-and-event-sourcing-oh-my.aspx</guid>
		<description><![CDATA[One of our alumni Karl blogged a request recently for folks to stop using mocks. Once upon a time I also made clear that I had a significant distrust of mocks. I&#8217;ve mellowed on that position over time, so I&#160;&#8230; <a href="http://codebetter.com/iancooper/2011/04/05/tell-dont-ask-mocks/">Continue&#160;reading&#160;<span class="meta-nav">&#8594;</span></a>]]></description>
			<content:encoded><![CDATA[<p>One of our alumni Karl blogged a request recently for folks to <a href="http://openmymind.net/2011/3/23/Stop-Using-Mocks">stop using mocks</a>. Once upon a time I also made clear that I had a <a href="http://codebetter.com/iancooper/2008/02/04/classicist-vs-mockist-test-driven-development/">significant distrust of mocks</a>. I&#8217;ve mellowed on that position over time, so I thought that I should explain why I have changed my opinion.</p>
<p>Perhaps it would be useful to give a summary of the discussion around this. Whilst I don&#8217;t think that you need to choose between being a classicist or mockist, I think that Martin Fowler&#8217;s article on <a href="http://martinfowler.com/articles/mocksArentStubs.html">Mocks Aren&#8217;t Stubs</a> is still a good starting point for understanding the debate, because it talks about the different types of <a href="http://xunitpatterns.com/Test%20Double.html">Test Double</a>, as well as outlining approaches to their usage. However, for my part, Martin does not highlight Meszaros&#8217;s key distinction between replacing <a href="http://xunitpatterns.com/indirect%20input.html">indirect inputs</a> and monitoring <a href="http://xunitpatterns.com/indirect%20output.html">indirect outputs</a>. Both mention the danger of <a href="http://xunitpatterns.com/Fragile%20Test.html#Overspecified%20Software">Over-specified Software</a> from Mocks, and Meszaros cautions to <a href="http://xunitpatterns.com/Principles%20of%20Test%20Automation.html#Use%20the%20Front%20Door%20First">Use the Front Door</a>. On the other side the proponents of mocks have always pointed to their admonition to<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.2927&amp;rep=rep1&amp;type=pdf"> Mock Roles Not Objects </a>{PDF}. <a href="http://www.growing-object-oriented-software.com/">Growing Object Oriented Software, Guided by Tests</a> (GooS) is probably the best expression of their technique today.</p>
<p>The key to understanding mocks to me was that the motivation behind Mocks, revealed in GooS is to support the <a href="http://pragprog.com/articles/tell-dont-ask">Tell Don&#8217;t Ask</a> principle. Tim Mackinnon notes that:</p>
<blockquote><p>In particular, I had noticed a tendency to add “getter” methods to our  objects to facilitate testing. This felt wrong, since it could be seen  as violating object-oriented principles, so I was interested in the  thoughts of the other members. The conversation was quite lively—mainly  centering on the tension between pragmatism in testing and pure  object-oriented design.</p></blockquote>
<p>For me understanding this goal for mock objects was something of a revelation in how I understood them to be used. Previously I had heard arguments that centered around the concept of a unit test needing isolation being the driver for use of mocking frameworks, but now I can see them as tools helping us avoid asking the object about its internal state.</p>
<p>Within any test we tend to follow the same pattern: set up the pre-conditions for the test, exercise the test and then check post-conditions. It&#8217;s the latter step that causes the pain for developers working towards a Tell Don&#8217;t Ask principle &#8211; how do I confirm the post-conditions without asking objects for their state.</p>
<p>This is a concern because objects should encapsulate state and expose behavior (whereas data structures have state but no behavior). This helps avoid <a href="http://en.wikipedia.org/wiki/Code_smell">feature envy</a>, where an objects collaborator holds the behavior of that object, in a set of calls to discover its state instead of asking the object to carry out the behavior on the state itself. This leads to coupling and makes our software more rigid (difficult to change) and brittle (likely to break when we do change it).</p>
<p>But this leads to the question: how do we confirm the post-conditions if we have no getters?</p>
<p>Mocks come into play where we decide to confirm is the behavior of the class-under-test through its interaction with its collaborators, instead of through getters checking the state. This is the meaning of behavior-based testing over state-based testing, although you might want to think of it as message-based testing, because what you really care about is the messages that you send to your collaborators.</p>
<p>Now when we talk about confirming behavior, recognize that in a Tell Don&#8217;t Ask style the objects we are interested in confirming our interaction with are the indirect outputs of the class under test, not the indirect inputs (classes that we get state from). In an ideal world of course, we won&#8217;t have many indirect inputs (we want Tell Don&#8217;t Ask remember), but most people will tend to be realistic about how far they can take a Tell Don&#8217;t Ask approach. Those indirect inputs may continue to be fakes, stubs, or even instances of collaborators created with TestDataBuilders or ObjectMothers. Of course the presence of a lot of them in our test setup is a smell that we have not followed a Test Don&#8217;t Ask approach.</p>
<p>It is also likely that many of our concerns about over-specified software with mocks often come from the coupling caused by these indirect inputs over indirect outputs. They are likely to reveal most about the implementation of the their collaborator. By contrast where we tell an object to do something for us, we likely let it hide the implementation details from us. Of course we need to beware of feature envy from calling a lot of setters instead of simply calling a method on the collaborator. This chattiness is again a feature envy design smell.</p>
<p>I have come to see an over-specified test when using mocks is not a problem of using mocks themselves, but mocks surfacing the problem that the class-under-test is not telling collaborators to perform actions at a granular enough level, or is asking too many questions of collaborators.</p>
<p>Finally one question that often arises here is: if we are just handing off to other classes, how do we know that the state was eventually correct i.e. the algorithm calculated the right value, or we stored the correct information. One catch-all option is to use a visitor, and pass it in so that the class-under-test can populate it with the state that you need to observe. Other options include observers that listen for changes. Of course at some point you need to consume the data from the visitor or the event sent to the observer &#8211; but the trick here is to use a data structure where it is acceptable to expose state (but not to have behavior).</p>
<p><a href="http://cqrsinfo.com/">CQRS</a> also has huge payoffs for a Tell Don&#8217;t Ask approach. Because your write side rarely needs to &#8216;get&#8217; the state, you can defer the question of where we consume the data &#8211; the read side does it, in the simplest fashion of just creating data structures directly. Event Sourcing also has benefits here, because you can confirm the changes to the state made to the object by examining the events raised by the class under test instead of asking the object for its state.</p>
]]></content:encoded>
			<wfw:commentRss>http://codebetter.com/iancooper/2011/04/05/tell-dont-ask-mocks/feed/</wfw:commentRss>
		<slash:comments>5</slash:comments>
		</item>
		<item>
		<title>TDD/BDD and the lean startup</title>
		<link>http://codebetter.com/iancooper/2011/03/08/tddbdd-and-the-lean-startup/</link>
		<comments>http://codebetter.com/iancooper/2011/03/08/tddbdd-and-the-lean-startup/#comments</comments>
		<pubDate>Tue, 08 Mar 2011 09:38:01 +0000</pubDate>
		<dc:creator>Ian Cooper</dc:creator>
				<category><![CDATA[Agile]]></category>
		<category><![CDATA[ATDD]]></category>
		<category><![CDATA[BDD]]></category>
		<category><![CDATA[STDD]]></category>
		<category><![CDATA[TDD]]></category>
		<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://codebetter.com/iancooper/?p=110</guid>
		<description><![CDATA[A post at Hacker News caused a little bit of a storm on Twitter, by questioning if a lean startup needed to use Test First or if it was an obstacle to delivery. Build the right thing, or avoid Rework&#160;&#8230; <a href="http://codebetter.com/iancooper/2011/03/08/tddbdd-and-the-lean-startup/">Continue&#160;reading&#160;<span class="meta-nav">&#8594;</span></a>]]></description>
			<content:encoded><![CDATA[<p>A post at <a title="Hacker News TDD" href="http://news.ycombinator.com/item?id=2240595">Hacker News</a> caused a little bit of a storm on Twitter, by questioning if a lean startup needed to use <a href="http://www.extremeprogramming.org/rules/testfirst.html">Test First</a> or if it was an obstacle to delivery.</p>
<p><strong>Build the right thing, or avoid Rework<br />
</strong></p>
<p>The poster complains that test first approaches don&#8217;t match with market driven evaluation of features:</p>
<blockquote><p>Most of the code in a lean startup are hypotheses that will be tested by the market, and possibly thrown out (even harder to rewrite test cases with slightly different requirements over writing from the beginning).</p></blockquote>
<p>First let&#8217;s look at a big reduction on the cost of delivery of software &#8211; building the right thing in the first place. Those people who come from a background in organizations with little or no process, often confuse Agile&#8217;s attack on process over software as support for having no process. It is not. In many ways classical agile is best understood in the context of what went before, heavyweight process, often waterfall such as SSADM. In these cases up to a third of the project&#8217;s time was spent in the design phase, before code was written. We don&#8217;t want the waste associated with that, but we do want <a href="http://www.infoq.com/articles/many-levels-planning-agile-project">just enough process</a> and we want it at the last responsible moment.</p>
<p>Test-First approaches focus on ensuring that we think about the acceptance criteria for the software we are about to build. The big danger we are trying to avoid is re-work. It is estimated that <a href="http://www.ebgconsulting.com/reqtproblems.php">50% of a project&#8217;s overrun costs come down to rework</a>. Creating a testable specification, through a Given-When-Then scenario or use case helps us to turn a woolly and vague requirement, such as we need to show how late a tube train is running, to something more concrete. We are forced to consider the edge cases, and the failure conditions. It is cheaper to consider these before the code is written, than through writing code and adjusting it afterwards. There will always be some rework, particularly around UX, but we can remove the obvious communication barriers by having a structured conversation around the requirements.</p>
<p>BDD and ATDD emphasize avoiding rework by ensuring that you have the acceptance criteria for a story defined before work begins. The process of defining those criteria, often through a story workshop is much of the value, the creation of  a shared understanding of how the product should work. Usually that workshop occurs in the preceding iteration, and takes about 10-15% of the time in that iteration. Whilst some folks consider it non-agile to work on the next iteration&#8217;s requirements, this really is the last responsible moment, because you may raise questions when discussing the acceptance detail for a story that take time to answer, and you don&#8217;t want to stall the line to answer them in an iteration.</p>
<p>So its important to understand that in this context Test-First is about reducing waste in the development cycle, not increasing it.</p>
<p>From <a href="http://www.extremeprogramming.org/rules/testfirst.html">Extreme Programming</a>:</p>
<blockquote><p>Creating a unit test helps a developer to really consider what needs to be done. Requirements are nailed down firmly by tests. There can be no misunderstanding a specification written in the form of executable code.<br />
<img src="http://www.extremeprogramming.org/images/pixel.gif" border="0" alt=" " width="45" height="4" align="bottom" /></p></blockquote>
<p>We still need to address the question of what will work in the marketplace; what if your feature is supposed to &#8220;test the water&#8221;. Usually, the concern here is whether testing is wasted effort if you decide to throw the feature away. Some of that depends on what we think the cost of working test first is &#8211; and I&#8217;ll get to that next, but an alternative to waste reduction here is to look at incremental development approaches. Figure out what the minimum marketable feature set is, and release that to gain feedback.</p>
<p>With subsequent iterations increment the feature set, if feedback suggests that it is successful. However, at the heart of this is that a good product depends on a good vision, cleanly and simply executed.</p>
<p>Think about the great products you know, they will tend to focus on doing one thing and doing it well, not being a jack of all trades. The scatter-gun approach tends to indicate a lack of product vision to me, not an effective strategy for uncovering what your users want. Build something simple, and wait for feedback on additional capabilities. But they have to buy your vision.</p>
<p>There are many emerging ideas on how to reduce the cost of discovering the market demand for new features, without building them. The <a href="http://www.pretotyping.org/">Pretotyping </a>guys are gaining some publicity, but there are also the <a href="http://www.nearfuturelaboratory.com/2009/03/17/design-fiction-a-short-essay-on-design-science-fact-and-fiction/">Design Fiction</a> crowd. Both talk about the step before even prototyping when you try to determine the viability of an idea. Recently at <a href="http://blog.huddle.net/">Huddle </a>we have been practicing <a href="http://thinkingphp.org/spliceit/docs/0.1_alpha/pages/ddd_info.html">Documentation Driven Development</a> in an effort to gain feedback on proposed <a href="http://code.google.com/p/huddle-apis/">API designs</a>, before we build them.</p>
<p><strong>Feedback, and the cost of TDD<br />
</strong></p>
<p>The poster on Hacker News complains of the time taken to write tests:</p>
<blockquote><p>There always seems to be about 3-5x as much code for a feature&#8217;s respective set of tests as the actual feature (just takes a lot of damn time to write that much code).</p></blockquote>
<p>It&#8217;s crazy talk to suggest that developers ship code without testing it in some way. Before TDD the best practices that I used to follow talked about spinning up the debugger and tracing every line of code that you had written to make sure it behaved as you expected. Spinning up the debugger and tracing is expensive, far more expensive than unit testing where you hope not to have to start a debugging session to implement a test (it&#8217;s usually a sign &#8211; to use Kent Beck&#8217;s stick shift analogy &#8211; that you are in the wrong gear in TDD and need to shift down by writing lower level tests).</p>
<p>In addition, getting this code to the point you can debug may be hard, requiring a number of steps to put the system in the right step. Unit tests are isolated from these concerns, which makes their context easier to establish, you have no such luck with manual tracing.</p>
<p>This problem only gets worse if you have to repeatedly spin up the debugger to trace through the code as you enhance it. Most folks break a problem down into parts to solve it, adding new capabilities. They are naturally incremental. So this means you need to perform this test run through repeatedly.</p>
<p>In addition, unit tests are not particularly new and have been a recommended practice for year. Many developers wrote self-testing code, that came with a simple harness to establish context and exercise the code, asserting on failure, before test first capitalized on the approach, but switched to writing tests first. Anyone who cares about their code being correct ought to be unit testing, and if you write the test last you are in the unenviable position of paying the cost to trace the code while you develop as well as writing a unit test after.</p>
<p>So usually, any halfway responsible developer is doing more work to prove his code is correct without test first rather than with.</p>
<p>The problem is that we tend to measure time to write the code, which seems slower with up-front tests, instead of time to fashion working code, which is often much faster with tests. Because of this we feel that test first takes longer. But we need to look at all the effort we expend to deliver the story end-to-end, which in this case tends to make dropping test first look like a local optima.</p>
<p>In addition, even if there was zero gain for test first, or a little worse, the side-affect of gaining a test suite that you can re-run exceeds any likely gains from not doing test first. So the barrier for tests to prove their value in cost to develop a feature is low.</p>
<p>The cost of ownership of tests can cause some concerns. The poster at Hacker News complains that:</p>
<blockquote><p>When we pivot or iterate, it seems we always spend a lot of time finding and deleting test cases related to old functionality (disincentive on iterating/pivoting, software is less flexible).</p></blockquote>
<p>One problem with &#8216;expensive&#8217; tests is often that implementers have not looked into best practice. Test code has its own rules, and is not a second class citizen. You need to both keep up to date with best practice to keep your test code in good shape. Too many teams fail to understand the importance of mastering the discipline of unit testing, in order to make it cost effective. As a starter, I would look at both <a href="http://xunitpatterns.com/">xUnit Test Patterns</a>, the <a href="http://www.pragprog.com/titles/achbd/the-rspec-book">RSpec book</a> and <a href="http://www.growing-object-oriented-software.com/">Growing Object Oriented Software Guided ByTests</a> if you need more advice on current practice.</p>
<p>I may write a longer post on this at some point, but my approach is to use a <a href="http://alistair.cockburn.us/Hexagonal+architecture">Hexagonal, or ports and adapters, architecture</a> with unit tests focused on exercising scenarios at the port, and diving deeper if you need to shift gears. Briefly though, one reason people find tests expensive to own is an over-reliance on <a href="http://xunitpatterns.com/Testcase%20Class%20per%20Class.html">Testcase class per class</a> instead of <a href="http://xunitpatterns.com/Testcase%20Class%20per%20Feature.html">Testcase per class Feature</a> or Testcase per Scenario. Remember that you care about putting your public API under test, which is usually the port level in a Hexagonal architecture, perhaps a number of top-level domain classes exercised in those ports. You may want to wrap internals when you need to shift down a gear, in Kent Beck&#8217;s terminology, because you are unsure of the road  ahead, but don&#8217;t succumb to the temptation to believe Test First is about testing every class in your solution. This will only become an obstacle to refactoring, because your changes will break tests, as you change the internal structure of your application, which makes testing itself feel as though it makes the application more brittle.</p>
<p>Often too many tests breaking is a sign that you have not encapsulated your implementation enough, exposing it all publicly, and you are over-specifying that implementation through tests.</p>
<p><strong>Acceptance and Unit Tests</strong></p>
<p>Let&#8217;s agree definitions. Acceptance tests confirm that a story is done-done whereas a unit test confirms that a part is correct. Or to put it another way unit tests confirm that we have built the software right, acceptance tests that we built the right software.</p>
<p>A lot of the above discussion focuses on unit tests, but when we get to acceptance tests the situation can get a little more complex. Acceptance testing may be automated, but it may also be manual and the trade off on when to automate is a more difficult one to make than when using unit testing.</p>
<p>Now the clear win tends to come when you have well-defined business rules that you want to automate. Provided you have a hexagonal, or ports and adapters architecture, you can usually test these at the port  &#8211; i.e. sub-cutaneously without touching the UX. So they tend to be similar to unit tests written at the same level, they just go end-to-end.</p>
<p>Automated tests tend to be a little more complex when they go end-to-end, because you have to worry about setting up data in the required state for the test to run. If you find your tests taking too long to write, you may need to break out common functionality such as setting up context in a <a href="http://www.natpryce.com/articles/000714.html">TestDataBuilder</a>. (The TestDataBuilder pattern is also useful in unit tests for setting up complex collaborating objects).</p>
<p>There is a misunderstanding that acceptance tests are about tooling, using <a href="http://fitnesse.org/">Fitnesse</a>, <a href="http://cukes.info/">Cucumber </a>or the like. It&#8217;s about agreeing acceptance criteria up front, the clarity of the story that is provided through that preventing rework, and the automation of the confirmation so that what is done stays done. The appropriate tooling depends on how you need to communicate the resulting tests &#8211; with a ports-and-adapters architecture you just need a need a testrunner to execute code, and that testrunner may be written in natural language or code depending on the audience.</p>
<p>Because much of the value to acceptance tests comes from defining done-done it would be possible to suggest that automating these tests gives less value back than the process of agreeing the test plan. However that suite of automated tests gives you quick regression testing support, and that supports your ability to re-write instead of refactor. Unit tests support preserving behaviour whilst refactoring, but this does not include changing the public interface; for that i.e. preserving the business rules while making large scale changes, you need some acceptance tests.</p>
<p>It is common for teams that do hit the bar of effective unit testing to fail at the bar of acceptance testing.</p>
<p>My previous experience working both with and without acceptance tests is that without acceptance tests we often find bugs creeping in the software that our unit test suite does not catch. This is especially true if our acceptance tests are our only integration tests. If we do no enforce the strict policy that existing acceptance tests must be green before check in, we often get builds that introduce regression issues, that may go uncaught for some period of time. It&#8217;s this long feedback loop, between an issue being created by a check in and then discovered that causes us significant issues &#8211; because we do not know what change led to the regression issue.</p>
<p>So by avoiding the cost of acceptance tests we risk the costs of the work to fix those regression defects and because the point between the defect being discovered and a developer making a check in long, they may cost more to fix than writing the acceptance tests would have done.</p>
<p>In environments where I have enforced acceptance tests being green before check-in the quality of software improved and when regression issues did emerge it was clear to the developer that thier changes had caused the issue, leading to faster resolution (and preserving the current test environments).</p>
<p>Of course this may depend on how long it takes to regression test your software. That situation seems to many even more marginal when we think about automating UI tests for acceptance. Often the cost of <a href="http://blog.objectmentor.com/articles/2010/01/04/ui-test-automation-tools-are-snake-oil">keeping pace with changes to the UI within tests can seem to be fruitless</a>. Particularly in early stages of a product the cost of creating and maintaining UX tests can seem to exceed their value. You can end up in a scenario where the cost of a manual regression test run is less than the cost of maintaining an automated test suite, and finds more defects.</p>
<p>This is especially true if you have a thin UI adapter layer over your ports, with acceptance tests against the ports catching your end-to-end issues. You already have plenty of behaviour preserving functionality around your business rules, so now the UI tests should really be only picking up UI defects (of course if you have a <a href="http://en.wikipedia.org/wiki/Magic_pushbutton">Magic Pushbutton</a> architecture all bets are off as you will can&#8217;t easily write tests without exercising the UI)</p>
<p>The problem becomes that this equation holds true, until it doesn&#8217;t. At a certain point the manual regression test costs become an obstacle to regular releases &#8211; simply put it takes you too long to confirm the software, and the cost of releasing rises to the point that you often defer it, because you don&#8217;t have sufficient value in a release compared to the regression testing effort.</p>
<p>At that point you will wish you had been building those automated UI tests all along.</p>
<p>I think there is a point where you need to begin UX automation, but it is hard to call the exact point when you begin doing that. It may depend on your ability to create adequate coverage through acceptance tests written against your ports, it may depend on the complexity of the UI. Up to now I think I have always started too early, regretted it and abandoned the effort, then resumed too later because of that. I need to learn more here before I can make a call on getting this right. Again I suspect that understanding best practice around this type of testing might help, and I&#8217;d be happy for anyone sharing links in the comments below.</p>
<p><strong>TDD/BDD and the Lean Startup</strong></p>
<p>Overall I think that there are two dangers to the simplistic position given by the original poster. The first is to mistake reducing the time taken to get to the point where you can run code, as reducing the time taken to get to the point where you can release code. Once you appreciate the two are not the same, and look at overall costs and benefits, the value of TDD/BDD becomes more compelling. The second is to mistake lack of understanding of best practice in TDD with a failure in the process itself. We have come a long way in the last few years in learning how to reduce the costs of writing and owning tests, and they should be judged in that context.</p>
]]></content:encoded>
			<wfw:commentRss>http://codebetter.com/iancooper/2011/03/08/tddbdd-and-the-lean-startup/feed/</wfw:commentRss>
		<slash:comments>7</slash:comments>
		</item>
		<item>
		<title>Alt.Next</title>
		<link>http://codebetter.com/iancooper/2011/01/16/alt-next/</link>
		<comments>http://codebetter.com/iancooper/2011/01/16/alt-next/#comments</comments>
		<pubDate>Sun, 16 Jan 2011 10:16:57 +0000</pubDate>
		<dc:creator>Ian Cooper</dc:creator>
				<category><![CDATA[altnetconf]]></category>
		<category><![CDATA[AltNetUK]]></category>

		<guid isPermaLink="false">http://codebetter.com/iancooper/?p=89</guid>
		<description><![CDATA[First off, apologies for the delay in writing this one; I can only plead the pressures of work at the end of last year. There are a number of blogs, in the pipe, which I hope you can expect to&#160;&#8230; <a href="http://codebetter.com/iancooper/2011/01/16/alt-next/">Continue&#160;reading&#160;<span class="meta-nav">&#8594;</span></a>]]></description>
			<content:encoded><![CDATA[<p>First off, apologies for the delay in writing this one; I can only plead the pressures of work at the end of last year. There are a number of blogs, in the pipe, which I hope you can expect to see soon.</p>
<p>However, I had promised to update people on what happened at that last alt.net conference in the UK, specifically because we declared that it would be the last alt.net conference, feeling that movement had run its course. That outcome was reported on Twitter, but I wanted to go into why, and talk about some ideas for what is next.</p>
<p>Alt.Next took place on <a title="Alt.Next" href="http://www.eventbrite.com/event/1014272717">19th\20th November 2010</a> and we were kindly hosted by <a title="Thoughtworks" href="http://www.thoughtworks.com/">Thoughtworks</a>. I gave the keynote on the condition, as I saw it on the state of the Alt.Net movement, something I have talked about <a title="Whither Alt.Net" href="http://codebetter.com/iancooper/2010/01/19/whither-alt-net/" target="_blank">here before on Codebetter</a>, but the history, as I saw it, is worth repeating here.</p>
<p><strong>A brief history of how the MS software community learned new ideas</strong></p>
<p>15 years ago, for the old hands amongst us, most our deep understanding came from books or from peers. There were rock star developers who we learned from. Just look at the Wrox publications of this period for the epitome of this rock star author movement with its, somewhat cringe-worthy, photos of geek celebrities embossed on its front covers. I can remember poring over copies of MFC Internals and Revolutionary Guide to MCF4 Programming with Visual C++.</p>
<p>But the problem was that knowledge was  ghettoized; most folks read the books they needed, and magazines like MSDN. There seemed to be very little cross pollination of ideas outside individuals moving jobs.</p>
<p>10 years ago, the Internet started to become a source of technical knowledge. Now there were <a href="http://en.wikipedia.org/wiki/Bulletin_board_system">BBS</a> systems before this, but I don&#8217;t think they had the same reach. In my recollection, the real explosion here  corresponded with the emergence of <a href="http://en.wikipedia.org/wiki/Java_%28software_platform%29">Java</a>. Java was cool and even places like The Well had Java forums where developers could gather and talk about this new language. It was either something to do with the paradigm shift caused by Java creating a new playing field, in which the old rock stars had to work to avoid looking like yesterday&#8217;s heroes, or the cross-fertilization as people from myriad former communities come together within Java; but somehow there seemed to be a greater sharing of knowledge. The masters were revealing their secrets to the journeymen and apprentices. It was a major shift in the availability of knowledge, and even books seemed to change to reflect that.</p>
<p>To my recollection this was the beginning of the first Windows Developer Diaspora. A lot of smart kids wrote leaving MS emails and forum posts and moved to Java. Plus ca change. Eventually MS reacted and moved to stop the diaspora with their own extensions to the <a href="http://en.wikipedia.org/wiki/Jvm">JVM</a>, and following that the <a href="http://en.wikipedia.org/wiki/Common_Language_Runtime">CLR</a>.</p>
<p>But the result of this explosion of knowledge, and the peering over the fence from some MS developers, looking to see what all this Java noise was about, led to a few of them encountering new ideas. New ideas like <a href="http://en.wikipedia.org/wiki/Object-relational_mapping">ORMs</a>, and <a href="http://en.wikipedia.org/wiki/Inversion_of_Control">IoC</a> containers. And some of them realized that the similarities between Java and C# gave them the ability to port some of these <a href="http://en.wikipedia.org/wiki/Open-source_software">OSS</a> projects into the .NET space to get the leg up on using them themselves.</p>
<p>In addition, as these MS developers looked to the Java community to understand best practices that might help them use their new frameworks, they stumbled across agile software engineering.</p>
<p>But&#8230;this cross fertilization did not seem to spread widely in the MS community. A lot of them were so busy fighting a rearguard action around <a href="http://en.wikipedia.org/wiki/Microsoft_Foundation_Class_Library">MFC</a> and <a href="http://en.wikipedia.org/wiki/Visual_Basic">VB</a> that they already saw .NET as an invader, let alone ideas from other communities. I think they were afraid of the &#8216;promiscuous&#8217; behavior of.NET developers who flirted with these new ideas; they threatened the rock star developers dominion. And that frustrated those &#8216;promiscuous&#8217; developers who were excited by the possibilities of these new ideas, who saw a better, faster way of working. But without widespread understanding of these ideas, adoption was always a struggle, and these &#8216;promiscuous&#8217; developers became angry that no one was spreading the good news of the better way to their colleagues and managers, forcing them to work with impoverished tools and practices.</p>
<p>And I think it was in this context, that in 2008 a number of &#8216;promiscuous&#8217; MVPs gather at MVP summit. Many are folks you would recognize as current of alumni CodeBetter bloggers. They clustered together through shared beliefs that the .NET community was blind and deaf to other practices, frameworks and patterns, and that MS was happy to let them remain in the dark. Their anger coalesced around the Entity Framework, which was seen as a poor alternative to Nhibernate, or even LINQ to SQL for folks that wanted to work in a new way.</p>
<p>Because they were associated with the EF rebellion they became knows as the NHibernate mafia at first. <a href="http://laribee.com/altnet">But David Laribee gave them a new name, and a manifesto from hacker culture with Alt.Net</a>.</p>
<p>Some may feel the Alt.Net movement succeeded, some may feel it failed. However, awareness of agile software engineering ideas like <a href="http://en.wikipedia.org/wiki/Solid_%28object-oriented_design%29">SOLID</a> or <a href="http://en.wikipedia.org/wiki/Test-driven_development">TDD</a> is far greater, as our tools like ORMs or IoC containers. When MVC frameworks began their rise with Rails, the .NET community furnished Monorail and later MS produced ASP.NET MVC. Flawed or not, I think that the movement promulgated greater understanding of why these ideas were important. Now more and more people can work environments where these ideas can be pushed and adopted. So I think the Alt.NET movement achieved something, and I don&#8217;t think it should be regretted.</p>
<p>But of late the movement seems stalled. The question is what does that mean for the future of Alt.NET.</p>
<p><strong>Our Discussions</strong></p>
<p>A lot of the conversation on the day at Alt.Next can be centered around two perceived problems with the Alt.NET movement:</p>
<ul>
<li>Messianic.NET</li>
<li>The New Orthodoxy</li>
</ul>
<p>Many attendees brought stories of developers for whom Alt.NET was a negative moniker, applied to someone who is pushy and arrogant; with the implication that their ideas were somehow ivory-tower, not for us, and impractical. There was a feel that the &#8216;missionary&#8217; nature of the Alt.NET community had done more to offend that inform, and had often been too messianic. No one likes to be made to feel stupid, but the Alt.NET community made a lot of people feel that way.</p>
<p>The danger of the new orthodoxy can be summed up as the belief that Alt.NET is really just a set of tools. A TDD framework, an IoC container, an ORM, and an MVC web framework. Hence the question from outside: given decent versions of all those, isn&#8217;t the problem over &#8211; can&#8217;t Alt.NET just go away? Even deeper the problem seems to be that its not just generic versions of these tools but NHibernate over EF, anything over Unity, etc. that are required to prove your credibility to the movement. There even seems to be an orthodoxy that anyone not using R# is poor software developer who cares little for his craft. &#8220;By their works ye shall know them,&#8221; seems to be the mantra of this witch-hunt.</p>
<p>I think that none of the attendees believed that this was what Alt.NET had originally meant. Indeed David&#8217;s proposition had been: &#8220;It’s not the tools, it’s the solution&#8230;When tools, practices, or methods become mainstream it’s time to get  contrarian; time to look for new ways of doing things; time to shake it  up&#8221;.</p>
<p>And I think for my part, and many others is that whilst Alt.NET should now be about <a href="http://en.wikipedia.org/wiki/NoSQL">NoSQL</a>, J<a href="http://goutamdey.com/2010/01/02/5-javascript-mvc-frameworks-2/">avascript MVC frameworks</a>, <a href="http://nodejs.org/">Node.js</a> etc. it seems mired in what was, not what is next, what we need to improve to be better. And in turn when those are done, it needs to move on.</p>
<p>And so we decided to abandon the Alt.NET moniker for future open space events.We want to move away from waging a war to spreading ideas. It is about cross-pollination with other communities, platforms. Going out to listen to other conversations and bringing that back into the .NET space. Innovating and taking those ideas out of the .NET space.</p>
<p>We&#8217;re not even sure what the &#8220;new thing&#8221; should be called, or even if naming it matters, and is not a mistake. Perhaps Alt.NET is dead as a movement; others may still find it useful to identify the like minded, but whatever you want to call it, I think its important to embrace the manifesto that David outlined long ago over what it became in between:</p>
<p>&#8220;Ralph Waldo Emerson wrote “there are always two parties; the  establishment and the movement.” If you’re ALT.NET, you’re in the  movement. You’re shaking out the innovation. When the movement fails,  stalls, or needs improving you’re there starting/finding/supporting that  next leap forward.&#8221;</p>
<p>For our purpose though, Alt.NET is dead. I sing its praises, but its time to find the next leap forward.</p>
<p><strong>The New Diaspora</strong></p>
<p>One postscript is that once again we seem to be experiencing a diaspora. Once again ideas outside the community burn very brightly. Ideas that cannot simply be ascribed to tools but to approaches and practices. In this case I believe it is partly the OSS ecosystem that non-MS software communities support so much better. Perhaps our new movement will truly be born from a reaction to this crisis, as much as Alt.NET was to the last.</p>
]]></content:encoded>
			<wfw:commentRss>http://codebetter.com/iancooper/2011/01/16/alt-next/feed/</wfw:commentRss>
		<slash:comments>4</slash:comments>
		</item>
		<item>
		<title>Lucene.NET could use the community&#8217;s support</title>
		<link>http://codebetter.com/iancooper/2010/11/02/lucene-net-could-use-the-community-s-support/</link>
		<comments>http://codebetter.com/iancooper/2010/11/02/lucene-net-could-use-the-community-s-support/#comments</comments>
		<pubDate>Tue, 02 Nov 2010 04:10:00 +0000</pubDate>
		<dc:creator>Ian Cooper</dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">/blogs/ian_cooper/archive/2010/11/02/lucene-net-could-use-the-community-s-support.aspx</guid>
		<description><![CDATA[I don&#8217;t usually do this kind of post, but following up on my comments in Diverse.NET about the state of OSS in the community I thought it would be worth pointing out that the Lucene.NET project is looking for help.&#160;&#8230; <a href="http://codebetter.com/iancooper/2010/11/02/lucene-net-could-use-the-community-s-support/">Continue&#160;reading&#160;<span class="meta-nav">&#8594;</span></a>]]></description>
			<content:encoded><![CDATA[<p>I don&#8217;t usually do this kind of post, but following up on my comments in <a href="/blogs/ian_cooper/archive/2010/10/15/diverse-net.aspx">Diverse.NET</a> about the state of OSS in the community I thought it would be worth pointing out that the<a href="http://mail-archives.apache.org/mod_mbox/lucene-lucene-net-user/201010.mbox/browser"> Lucene.NET</a> project is<a href="http://codeclimber.net.nz/archive/2010/11/01/Lucene-Net-needs-your-help-or-it-will-die.aspx"> looking for help</a>. Anecdotally I know that a lot of people use Lucene.NET in London, so I suspect that situation is mirrored in the wider community. A lot of the love for Lucene.NET probably comes from its usage in <a href="http://ayende.com/Blog/archive/2009/05/03/nhibernate-search-again.aspx">NHibernate Search</a>. <a href="http://ravendb.net/">RavenDB </a>uses Lucene.NET internally. Lucene (and related pieces like Solr) are &#8216;big beasts&#8217; in the search field, must as Hibernate is in the ORM field, and having a working port gives us easy access to this technology in the same way NHibernate gave us a first class ORM. So I think its an important piece of the ecosystem for us to support. For those looking for a little more <a href="http://sleepoverrated.com/">Scott Cowan</a> gave a presenation on Lucene at Skillsmatter that you can check out <a href="http://skillsmatter.com/podcast/open-source-dot-net/how-to-make-a-search-engine-with-lucenedot-net">here</a>.&nbsp;</p>
<p> It&#8217;s probably worth reading through the thread on the mailing list archives to get a feel for the conversation there as to the direction the project is headed particularly this <a href="http://mail-archives.apache.org/mod_mbox/lucene-lucene-net-user/201011.mbox/browser">post</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://codebetter.com/iancooper/2010/11/02/lucene-net-could-use-the-community-s-support/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Diverse.NET</title>
		<link>http://codebetter.com/iancooper/2010/10/15/diverse-net/</link>
		<comments>http://codebetter.com/iancooper/2010/10/15/diverse-net/#comments</comments>
		<pubDate>Fri, 15 Oct 2010 08:30:00 +0000</pubDate>
		<dc:creator>Ian Cooper</dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">/blogs/ian_cooper/archive/2010/10/15/diverse-net.aspx</guid>
		<description><![CDATA[Rob Conery has an&#160; interesting piece about adopting an open source project to encourage more OSS development in the .NET space. It is something we need. But I have one slight issue to pick with Rob&#8217;s comments. The .NET community&#160;&#8230; <a href="http://codebetter.com/iancooper/2010/10/15/diverse-net/">Continue&#160;reading&#160;<span class="meta-nav">&#8594;</span></a>]]></description>
			<content:encoded><![CDATA[<p>Rob Conery has an&nbsp; interesting piece about <a href="http://blog.wekeroad.com/thoughts/making-bacon-from-lemonade">adopting an open source project</a> to encourage more OSS development in the .NET space. It is something we need. But I have one slight issue to pick with Rob&#8217;s comments. The .NET community didn&#8217;t wait 7 years for MS to implement it for us. <a href="/blogs/sebastien_lambla/archive/2010/10/15/building-polyglot-packages-for-openwrap-and-nupack.aspx">Seb </a>went out there and wrote <a href="http://github.com/openrasta/openwrap/wiki">OpenWrap</a>. What happened? Well it looks like outside the London .NET community folks were pretty much unaware of it it. Seb tried hard to publicize it, there is an interview about it on InfoQ back in <a href="http://www.infoq.com/news/2010/05/OpenWrap">May</a>. We hosted a session on it at <a href="http://skillsmatter.com/podcast/open-source-dot-net/packaging-with-openwrap">LDNUG</a>, and JetBrains sponsored an evening with a presentation at <a href="http://devconlondon.com/">DevCon London</a>. But it just never seemed to get the wider-audience it needed to break out of those closed circles. Many people looked at <a href="http://nu.wikispot.org/Front_Page">NuProj</a> as well. But even that project never seem to cross out of a small circle. (EDIT: And before that was <a href="http://code.google.com/p/hornget/">Horn</a> &#8211; thanks Paul &#8211; and even <a href="https://launchpad.net/coapp">CoApp</a>)</p>
<p>But MS entering the fray with <a href="http://nupack.codeplex.com/">nupack</a> has practically obliterated a &#8216;from the community&#8217; solution and created the 300ibs gorilla of an MS sponsored one. That may not be the &#8216;fault&#8217; of MS, their intent may be entirely good, but the road to hell is often paved with good intentions. By stepping in MS may have felt it was helping the community to resolve the issue. All it did was simply remove the ability of the community to solve the issue. I understand that nupack is set up as an OSS project, but by &#8216;doing it for us&#8217; they removed the community&#8217;s responsibility to solve the problem. So they have not aided the growth of community that addresses these issues. One of the hardest lessons to learn when leading or mentoring is to let others &#8216;do it for themselves&#8217; even if you could do it faster and sooner. Unless you give others the chance to try (and fail) you won&#8217;t get growth and you will always be doing it yourself.</p>
<p>The stark reality is this &#8211; the package management issue could have been resolved without MS help through OpenWrap (or NuProj). I don&#8217;t see significant technical advantage from NuPack over OpenWrap (it might win some adoption through a UI). I don&#8217;t know NuProj well enough to comment.</p>
<p>That would have given the opportunity for the community to understand it could solve its problems. If MS had publicized those OSS solutions with ASP.NET MVC 3 then it would have also given enormous publicity to community efforts to solve its own problems.</p>
<p>I don&#8217;t, before anyone suggests that, want to denigrate the effort put in by the team working on NuPack. I just wish that it did not come with the wieght of being the MS solution to this issue.</p>
<p>What MS could have done was to call for folks to tell them about OSS package managers, worked with those projects to define some kind of open package standard, and then communicated to the wider development community what the standard was and who implemented it. For sure, if the community failed it might have helped to kickstart some projects with sample code, but by creating an MS solution, they pushed aside existing community efforts. I would like to think that the community gets behind the best package manager and that we chose between OpenWrap, NuPack or whatever based on what works</p>
<p>There does seem a lack of willingness within the .NET community to engage with a diverse ecosystem over a monoculture.In the long term this is not good for us. It does not drive the kind of innovation a successful development platform needs. A monoculture is no more positive for MS than others in this regard. I also think that the problem is sufficient that it does not only impact MS, but a lot of commercial outfits in the .NET space. I am sure there are shops that don&#8217;t use R# or CodeRush, because its not MS. There are plenty of commercial frameworks and libraries that would benefit from a widening of people&#8217;s purchasing choices. (And sure I would like a lot of tools and frameworks that are commercial to go open source and free for private use like <a href="http://ravendb.net/">RavenDB </a>or <a href="http://www.nservicebus.com/">NServiceBUS </a>and licensed for commercial, because I believe it is easier to make choices around something that you can try before buying and support if the vendor fails, but diversity is diversity regardless of the model).</p>
<p>Somehow we need to encourage more diversity in the .NET space, in tools, languages, people. I understand that it&#8217;s a big challenge. We need IronRuby and IronPython on the platform for that diversity. We need folks to understand that they could look&nbsp; Boo too. We need folks to understand there is not just ASP.NET MVC but <a href="http://trac.caffeine-it.com/openrasta">OpenRasta</a>, <a href="http://www.castleproject.org/MonoRail/">Monorail</a>, and <a href="http://wiki.fubumvc.com/Main_Page">FubuMVC</a>. We need folks to know not just about Entity Framework and LINQ-To-SQL, but also <a href="http://nhforge.org/Default.aspx">NHibernate</a>, <a href="http://www.subsonicproject.com/">Subsonic</a>, or even <a href="http://www.devexpress.com/Products/NET/ORM/">XPO</a>. It&#8217;s not about favoritism but creating an ecosystem in which different solutions can compete.</p>
<p>If Alt.Net was yesterday&#8217;s war cry maybe Diverse.NET is tomorrow&#8217;s.</p>
<p>The tragedy is that only MS probably has the reach to the .NET community to encourage engagement with an ecosystem over a monoculture. And as MS own the monoculture, can they be convinced it lies in their best interest to support it?</p>
<p>&nbsp;</p>
]]></content:encoded>
			<wfw:commentRss>http://codebetter.com/iancooper/2010/10/15/diverse-net/feed/</wfw:commentRss>
		<slash:comments>49</slash:comments>
		</item>
		<item>
		<title>BDD, Feature Injection (and the Whirlpool)</title>
		<link>http://codebetter.com/iancooper/2010/06/15/bdd-feature-injection-and-the-whirlpool/</link>
		<comments>http://codebetter.com/iancooper/2010/06/15/bdd-feature-injection-and-the-whirlpool/#comments</comments>
		<pubDate>Tue, 15 Jun 2010 13:30:00 +0000</pubDate>
		<dc:creator>Ian Cooper</dc:creator>
				<category><![CDATA[ATDD]]></category>
		<category><![CDATA[BDD]]></category>
		<category><![CDATA[Behavior Specification]]></category>
		<category><![CDATA[STDD]]></category>
		<category><![CDATA[TDD]]></category>

		<guid isPermaLink="false">/blogs/ian_cooper/archive/2010/06/15/bdd-feature-injection-and-the-whirlpool.aspx</guid>
		<description><![CDATA[I have started to feel comfortable enough about our BDD practice to begin presenting on the techniques. Liz Keogh came to my last presentation at London .Net Developers Group and suggested that I needed to look at Feature Injection as&#160;&#8230; <a href="http://codebetter.com/iancooper/2010/06/15/bdd-feature-injection-and-the-whirlpool/">Continue&#160;reading&#160;<span class="meta-nav">&#8594;</span></a>]]></description>
			<content:encoded><![CDATA[<p>I have started to feel comfortable enough about our <a href="/blogs/ian_cooper/archive/2010/04/09/where-we-are-with-acceptance-testing-and-our-bdd-journey-today.aspx">BDD<br />
practice</a> to begin presenting on the techniques. <a href="http://lizkeogh.com/">Liz Keogh</a> came to my last<br />
presentation at London .Net Developers Group and suggested that I needed to<br />
look at <a href="http://www.limitedwipsociety.org/tag/feature-injection/">Feature<br />
Injection</a> as part of BDD practice now. Chris Matt&#8217;s comics are a good<br />
starting point for Feature Injection.</p>
<p>I&#8217;ve not tried Feature Injection, but think its definitely worth a look for<br />
anyone using BDD. I&#8217;m keen to get a better understanding of how it might modify<br />
our &#8216;today&#8217; practice. This is just really some early notes on what I know now.</p>
<p><b>Stakeholder Stories<br /></b></p>
<p>Liz Keogh has a useful article on how sees the overall process of BDD when<br />
using Feature Injection over at <a href="http://www.infoq.com/articles/pulling-power">InfoQ</a>. The shift in<br />
how user stories are phrased seems to be one key to understanding this.<br />
Business Value is now being recognized as a key driver for why we are<br />
delivering a piece of work. So we re-phrase the classic Role-Goal-Motivation<br />
User Story as:</p>
<p>As a &lt;role&gt;</p>
<p>I want to &lt;goal&gt;</p>
<p>So that &lt;motivation&gt;</p>
<p>with </p>
<p>In order to &lt;achieve some outcome which<br />
contributes to the vision&gt;<br />
As a &lt;stakeholder&gt;<br />
I want &lt;some other stakeholder&gt; &lt;to do, use or be restricted by<br />
something&gt;</p>
<p>Liz calls these <i>stakeholder stories</i> to emphasize the difference from user stories.</p>
<p><b>Distilling the Vision</b></p>
<p>Notice how the outcome, the reference to the vision, why are we building<br />
this software now comes first over the goal. We emphasize that it is not enough<br />
to say what, but also to say why. How does this story help us meet the vision?<br />
When I talk about BDD I often shorthand it as &#8220;Build the right<br />
software&#8221; in comparison to TDD which helps us &#8220;Build the software<br />
right&#8221;. Feature Injection seems to take this process one step further by<br />
saying that it is not enough to restrict the developers to only building<br />
software that satisfies a specification, but also to ensure that those<br />
specifications are authored only for those stories that deliver on the vision.<br />
So where does the vision come from? In DDD terms the Vision results from the Distillation<br />
process. Identify the core domain and produce a Core Domain Vision statement.<br />
So the technique of Feature Injection reinforces DDD&#8217;s Distillation technique.<br />
Both Liz&#8217;s article and Chris Matt&#8217;s comics identify some standard techniques<br />
for helping to distil what is core domain from what is not. Given that vision<br />
the stakeholders then work to identify the broad features of what need to be<br />
delivered. User stories come from decomposing these features. </p>
<p>Our software lifecycle tends to begin with Iteration -1 in which the Big<br />
Plan, how we think we might build some software to satisfy a Vision is taken to<br />
sponsors. At that point we have only the results of Distillation (what DDD<br />
calls a Distillation document) and enough understanding of the Feature Set we<br />
need to provide a rough costing (folks want to know the order of the costs to<br />
understand whether the business value justifies the cost &#8211; software is an<br />
economic proposition). We then move to Iteration 0 or project Chartering, where<br />
we might capture initial stories and produce what Crystal calls an <a href="http://iancooper.spaces.live.com/blog/cns!844BD2811F9ABE9C!527.entry">Exploratory<br />
360</a>.</p>
<p>What I like most about Feature Injection at first reading&nbsp; is this<br />
explicitness with which it highlights what happens in Iteration -1 and<br />
Iteration 0. These are no longer &#8216;locked in the attic&#8217; in agile process terms,<br />
but brought to the fore as a vital part of the methodology. I have come across<br />
too many teams that have no idea about Iteration -1 or Iteration 0, push<br />
straight into Iteration 1 and build the wrong thing. So any process that<br />
highlights the need for this to occur gets a thumbs up from me.</p>
<p><b>What is Business Value</b>?</p>
<p>If we are suggesting that our focus is on business value, how do we assess<br />
value? Chris Matt&#8217;s admonition to focus on delivering business value over<br />
features is something about Feature Injection that I really warm to, as again<br />
and again we try to point to the fact that the most important metric for us is<br />
business value. In particular Feature Injection suggests &#8216;popping the why<br />
stack&#8217; until you can arrive at a definition of business value that meets Andy<br />
Poll&#8217;s advice that &#8216;A project creates business value when it increases or<br />
protects revenue, or reduces costs in alignment with the strategy of the<br />
organization&#8217;. That&#8217;s pith because it not only identifies revenue or cost as<br />
the driver, but alignment with business strategy. For Chris, the place to look<br />
for business value is in the outputs of the system. It is the outputs that you<br />
can measure for business value. So once we identify an output, we &#8216;pop the<br />
stack&#8217; to find its business value.</p>
<p><b>Whither Use Cases?</b></p>
<p>Of late I have been wondering about the correspondence between examples and<br />
use cases and questioning whether as we emphasize modelling from examples that<br />
use cases did not have it right all along. So it is interesting in Liz&#8217;s<br />
article to pick up on the same question from Eric Evans, and respond that<br />
&#8220;You can&#8217;t ask a business person for a use case unless they&#8217;re already<br />
technical enough to know what a use case is. But you can ask them for an<br />
example&#8221;. That implies that they are two different approaches to the same<br />
goal, but the user is placed in the driving seat with examples. It&#8217;s a good<br />
explanation of the difference.</p>
<p><b>Intersection with DDD</b></p>
<p>Interestingly enough at Domain Driven Design Exchange, Gojko Adzic presented<br />
on the intersection of BDD and DDD and feature injection was at the heart of<br />
how he saw the two corresponding. You can see Gojko&#8217;s presentation <a href="http://skillsmatter.com/podcast/design-architecture/ddd-tdd-bdd">here</a>.<br />
Gojko&#8217;s main assertion, as I understand it, is that having used <a href="http://domaindrivendesign.org/node/104">Distillation</a> (figuring<br />
out what is <a href="http://domaindrivendesign.org/node/99">Core Domain</a> where you<br />
expend modelling effort, as opposed a Supporting Domain, where you don&#8217;t) to<br />
decide what to build we hold <a href="http://www.acceptancetesting.info/key-ideas/specification-workshop/">Specification<br />
Workshop</a> to drive out the scope using Feature Injection. We then model<br />
using to the scenarios outlined for the stories to create a <a href="http://domaindrivendesign.org/node/132">Ubiquitous Language</a>. At<br />
the same time Eric Evans spoke about his perspective on Agile and DDD, looking<br />
at DDD&#8217;s need to produce a Ubiquitous Language and asking if that was in<br />
conflict with Agile preference for &#8220;Last Responsible Moment&#8221; over<br />
BDUF. Again the solution space seemed to me to focus on modelling from<br />
examples, with enough modelling up front, followed by further modelling as the<br />
model is stressed. You can see Eric&#8217;s talk <a href="http://skillsmatter.com/podcast/design-architecture/folding-together-ddd-agile">here</a>,<br />
which includes a prototypical discussion of his Whirlpool approach.</p>
<p><b>Is there too much focus on the UI here though?</b></p>
<p>One of my concerns about the process as written is some of the focus on UI.<br />
I understand the principle that the user consumes through the UI and it is<br />
therefore the only valid test of the system, but this overlooks the extent to<br />
which the UI is in flux during the early part of development. Indeed the<br />
approach identifies that we need to release early to get feedback on the UI and<br />
revise. Those revisions cause issues for tests that are focused on the UI,<br />
because as the UI changes, the tests break. Rick Mugridge in <i>Fit for<br />
Developing Software</i> is to test the business rules, not the workflow.
 </p>
<p>Now I don&#8217;t want folks to get the wrong impression here. We product a Balsamiq mockup of our user interface as part of the discussion on any story, often as part of our describe step, and we iterate over it. So I am certainly not saying that the UI is not vital to successful delivery to the customer or to understanding what is to be delivered. However I am more cautious around automate testing that focuses on the UI as the driver for development. I would rather drive from the rules than a UI focused test. We still have story tests to help &#8216;build the right thing&#8217;, in Fitnesse (replace with Cucumber, Fit, Slim, Specflow, StoryTeller etc) but they are<a href="http://xunitpatterns.com/Layer%20Test.html"> subcutaneous tests</a>. Much of our UI testing is via Exploratory Testing, and automated UI suites check only stable areas and act as smoke tests for regression issues.</p>
<p><b>Summary</b></p>
<p>All in all, very interesting stuff, that I will be looking into more as I try to refine my BDD practice</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
]]></content:encoded>
			<wfw:commentRss>http://codebetter.com/iancooper/2010/06/15/bdd-feature-injection-and-the-whirlpool/feed/</wfw:commentRss>
		<slash:comments>6</slash:comments>
		</item>
	</channel>
</rss>

<!-- Dynamic page generated in 0.449 seconds. -->
<!-- Cached page generated by WP-Super-Cache on 2012-11-09 23:23:44 -->
